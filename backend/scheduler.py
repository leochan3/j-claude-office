"""
Automated Job Scraping Scheduler

This module handles automatic daily job scraping for all active target companies.
It runs as a background service and can be configured through environment variables.
"""

import asyncio
import schedule
import time
import threading
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any
import os
import logging
import json
import smtplib
import csv
import io
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
from sqlalchemy.orm import Session

from database import SessionLocal, TargetCompany, ScrapingRun, UserAutoscrapingConfig, User
from job_scraper import job_scraper
from models import BulkScrapingRequest
from daily_job_review import daily_job_reviewer
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoScrapingScheduler:
    """Handles automatic daily job scraping for target companies."""
    
    def __init__(self):
        self.job_scraper = job_scraper  # Use the global instance
        self.is_running = False
        self.scheduler_thread = None
        
        # Configuration from environment variables
        self.enabled = os.getenv("AUTO_SCRAPING_ENABLED", "true").lower() == "true"
        self.schedule_time = os.getenv("AUTO_SCRAPING_TIME", "20:55")  # Default: 8:55 PM
        self.max_results_per_company = int(os.getenv("AUTO_SCRAPING_MAX_RESULTS", "100"))
        self.default_search_terms = os.getenv("AUTO_SCRAPING_SEARCH_TERMS", "").split(",")
        self.default_search_terms = [term.strip() for term in self.default_search_terms if term.strip()]
        
        if not self.default_search_terms:
            self.default_search_terms = ["clinical", "medical", "research", "regulatory", "quality", "scientist", "manager", "director", "analyst", "biomedical"]
        
        # Email notification configuration
        self.email_enabled = os.getenv("EMAIL_NOTIFICATIONS_ENABLED", "false").lower() == "true"
        self.smtp_server = os.getenv("SMTP_SERVER", "smtp.gmail.com")
        self.smtp_port = int(os.getenv("SMTP_PORT", "587"))
        self.email_user = os.getenv("EMAIL_USER", "")
        self.email_password = os.getenv("EMAIL_PASSWORD", "")
        self.notification_email = os.getenv("NOTIFICATION_EMAIL", "")

        # OpenAI configuration for AI relevance evaluation
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        self.openai_client = None
        if self.openai_api_key and self.openai_api_key != "your_openai_api_key_here":
            try:
                self.openai_client = OpenAI(api_key=self.openai_api_key)
                logger.info("OpenAI client initialized for AI relevance evaluation")
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAI client: {e}")
        else:
            logger.info("OpenAI API key not configured - AI relevance evaluation will be skipped")

        logger.info(f"AutoScrapingScheduler ready - {'Enabled' if self.enabled else 'Disabled'} at {self.schedule_time}")
    
    def get_active_companies(self) -> List[TargetCompany]:
        """Get target companies for scraping based on saved defaults or all active companies."""
        db = SessionLocal()
        try:
            # First, try to load companies from scraping defaults
            default_companies = self._get_default_company_names()
            
            if default_companies:
                # Use specific companies from defaults
                companies = []
                for company_name in default_companies:
                    # Try exact match first
                    company = db.query(TargetCompany).filter(
                        TargetCompany.name.ilike(company_name),
                        TargetCompany.is_active == True
                    ).first()
                    
                    # If not found, try partial match
                    if not company:
                        company = db.query(TargetCompany).filter(
                            TargetCompany.name.ilike(f"%{company_name}%"),
                            TargetCompany.is_active == True
                        ).first()
                    
                    if company:
                        companies.append(company)
                        logger.info(f"âœ… Found target company: {company.name}")
                    else:
                        # Auto-create missing company
                        logger.info(f"ðŸ”§ Company '{company_name}' not found in database, creating it automatically...")
                        new_company = self._create_target_company(db, company_name)
                        if new_company:
                            companies.append(new_company)
                            logger.info(f"âœ… Created and added target company: {new_company.name}")
                        else:
                            logger.warning(f"âŒ Failed to create company: {company_name}")
                
                if companies:
                    logger.info(f"ðŸŽ¯ Using {len(companies)} specific target companies from defaults")
                    return companies
                else:
                    logger.warning("âŒ No valid companies found from defaults, falling back to all companies")
            
            # Fallback: use all active companies
            companies = db.query(TargetCompany).filter(
                TargetCompany.is_active == True
            ).all()
            logger.info(f"Found {len(companies)} active target companies (using all companies)")
            return companies
        finally:
            db.close()
    
    def _get_default_company_names(self) -> List[str]:
        """Get default company names from scraping defaults file."""
        try:
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    companies = data.get('companies', [])
                    if companies:
                        logger.info(f"ðŸ“‹ Loaded {len(companies)} default companies: {', '.join(companies)}")
                        return companies
        except Exception as e:
            logger.error(f"Error loading default companies: {e}")
        
        return []
    
    def _create_target_company(self, db: Session, company_name: str) -> Optional[TargetCompany]:
        """Create a new target company in the database with sensible defaults."""
        try:
            # Get default search terms from config
            default_search_terms = self._get_default_search_terms()
            
            # Create new company with intelligent defaults
            new_company = TargetCompany(
                name=company_name.title(),  # Capitalize properly
                display_name=company_name.title(),
                preferred_sites=["indeed", "linkedin", "glassdoor"],
                search_terms=default_search_terms,
                location_filters=["USA", "United States"],
                is_active=True
            )
            
            db.add(new_company)
            db.commit()
            db.refresh(new_company)
            
            logger.info(f"ðŸ¢ Auto-created target company: {new_company.name}")
            logger.info(f"   Search terms: {new_company.search_terms}")
            logger.info(f"   Sites: {new_company.preferred_sites}")
            
            return new_company
            
        except Exception as e:
            logger.error(f"Failed to create target company '{company_name}': {e}")
            db.rollback()
            return None
    
    def _get_default_hours_old(self) -> int:
        """Get default hours_old from scraping defaults file."""
        try:
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    hours_old = data.get('hours_old')
                    if hours_old:
                        logger.info(f"â° Loaded default hours_old: {hours_old} hours ({hours_old/24:.1f} days)")
                        return hours_old
        except Exception as e:
            logger.error(f"Error loading default hours_old: {e}")
        
        # Default fallback
        return 168  # 7 days
    
    def _get_default_results_per_company(self) -> int:
        """Get default results_per_company from scraping defaults file."""
        try:
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    results = data.get('results_per_company')
                    if results:
                        logger.info(f"ðŸ”¢ Loaded default results_per_company: {results}")
                        return results
        except Exception as e:
            logger.error(f"Error loading default results_per_company: {e}")
        
        # Fallback to environment variable or default
        return self.max_results_per_company

    def _get_default_location(self) -> str:
        """Get default location from autoscraping config file."""
        try:
            # First try autoscraping config file (UI settings)
            config_file = "autoscraping_config.json"
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    data = json.load(f)
                    location = data.get('location')
                    if location:
                        logger.info(f"ðŸ“ Loaded default location: {location}")
                        return location

            # Fallback to scraping defaults file
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    locations = data.get('locations')
                    if locations and isinstance(locations, list) and locations:
                        location = locations[0]  # Use first location
                        logger.info(f"ðŸ“ Loaded fallback location: {location}")
                        return location
        except Exception as e:
            logger.error(f"Error loading default location: {e}")

        # Default fallback
        return "USA"

    def _get_default_distance(self) -> int:
        """Get default distance from autoscraping config file."""
        try:
            # First try autoscraping config file (UI settings)
            config_file = "autoscraping_config.json"
            if os.path.exists(config_file):
                with open(config_file, 'r') as f:
                    data = json.load(f)
                    distance = data.get('distance')
                    if distance:
                        logger.info(f"ðŸŽ¯ Loaded default distance: {distance} miles")
                        return distance
        except Exception as e:
            logger.error(f"Error loading default distance: {e}")

        # Default fallback
        return 25

    def _get_scoring_config(self) -> dict:
        """Get scoring configuration from scraping defaults file."""
        try:
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    
                    # Support both single keyword (backward compatibility) and multiple keywords
                    keywords = []
                    if 'scoring_keywords' in data and isinstance(data['scoring_keywords'], list):
                        keywords = [kw.strip() for kw in data['scoring_keywords'] if kw.strip()]
                    elif 'scoring_keyword' in data and data['scoring_keyword']:
                        keywords = [data['scoring_keyword'].strip()]
                    
                    config = {
                        'scoring_keywords': keywords,
                        'expected_salary': data.get('expected_salary', 0)
                    }
                    logger.info(f"ðŸ”§ Loaded scoring config: keywords={keywords}, salary={config['expected_salary']}")
                    return config
        except Exception as e:
            logger.error(f"Error loading scoring config: {e}")
        
        return {'scoring_keywords': [], 'expected_salary': 0}
    
    def calculate_relevance_score(self, job: Dict[str, Any], search_term: str, expected_salary: int = None) -> int:
        """Calculate job relevance score using the same logic as the main app."""
        score = 0
        if not search_term or not search_term.strip():
            return 0
        
        # Get job details
        title = (job.get('title', '') or '').lower().strip()
        description = (job.get('description', '') or '').lower().strip()
        company = (job.get('company', '') or '').lower().strip()
        search = search_term.lower().strip()
        
        # Split search term into words, filter out common stop words
        stop_words = {'and', 'or', 'the', 'a', 'an', 'in', 'at', 'for', 'with', 'by', 'to', 'of', 'from'}
        search_words = [word for word in search.split() if len(word) > 1 and word not in stop_words]
        
        if not search_words:
            return 0
        
        # 1. Exact title match (highest score)
        if title == search:
            score += 100
        
        # 2. Title contains full search phrase
        if search in title:
            score += 80
        
        # 3. All search words in title (high score)
        title_words = title.split()
        title_words_matched = [sw for sw in search_words if any(sw in tw or tw in sw for tw in title_words)]
        
        if len(title_words_matched) == len(search_words):
            score += 60  # All words found
        elif title_words_matched:
            score += (len(title_words_matched) / len(search_words)) * 40  # Partial match
        
        # 4. Individual word matches in title (position matters)
        for search_word in search_words:
            for title_index, title_word in enumerate(title_words):
                if search_word in title_word:
                    # Earlier position in title = higher score
                    position_bonus = max(0, 10 - title_index * 2)
                    score += 15 + position_bonus
                elif title_word in search_word and len(title_word) > 2:
                    score += 8  # Partial word match
        
        # 5. Description matches (lower weight)
        if description:
            for search_word in search_words:
                matches = description.count(search_word)
                if matches:
                    score += min(matches * 5, 20)  # Cap at 20 points per word
            
            # Bonus for search phrase in description
            if search in description:
                score += 15
        
        # 6. Company name relevance (small bonus)
        if company:
            for search_word in search_words:
                if search_word in company:
                    score += 5
        
        # 7. Job type matching bonus
        job_type = (job.get('job_type', '') or '').lower()
        if job_type and (search in job_type or ('full' in job_type and 'full' in search)):
            score += 8
        
        # 8. Salary matching with rewards and penalties
        if expected_salary and expected_salary > 0:
            min_salary = job.get('min_amount')
            max_salary = job.get('max_amount')
            
            if min_salary and max_salary:
                # Calculate median salary for the job
                median_job_salary = (min_salary + max_salary) / 2
                salary_diff = abs(expected_salary - median_job_salary)
                percentage_diff = salary_diff / expected_salary
                
                # Salary scoring with rewards and penalties
                if percentage_diff <= 0.15:
                    salary_score = 30  # Within 15% - excellent match
                elif percentage_diff <= 0.25:
                    salary_score = 25  # Within 25% - very good match
                elif percentage_diff <= 0.40:
                    salary_score = 10  # Within 40% - acceptable match
                elif percentage_diff <= 0.60:
                    salary_score = -15  # 40-60% - penalty for poor match
                elif percentage_diff <= 0.80:
                    salary_score = -30  # 60-80% - bigger penalty
                else:
                    salary_score = -50  # Above 80% - major penalty
                
                score += salary_score
                
            elif min_salary or max_salary:
                # Only one salary bound available
                available_salary = min_salary or max_salary
                salary_diff = abs(expected_salary - available_salary)
                percentage_diff = salary_diff / expected_salary
                
                # Same scoring logic for partial data
                if percentage_diff <= 0.15:
                    salary_score = 30
                elif percentage_diff <= 0.25:
                    salary_score = 25
                elif percentage_diff <= 0.40:
                    salary_score = 10
                elif percentage_diff <= 0.60:
                    salary_score = -15
                elif percentage_diff <= 0.80:
                    salary_score = -30
                else:
                    salary_score = -50
                
                score += salary_score
            else:
                # No salary data - give small bonus as neutral reward
                score += 10
        
        # 9. Recency bonus (newer posts get higher ranking)
        if job.get('date_posted'):
            try:
                from datetime import datetime
                post_date = datetime.fromisoformat(str(job['date_posted']).replace('Z', '+00:00'))
                now = datetime.now(post_date.tzinfo)
                days_since_posted = (now - post_date).days
                
                # Recency bonus: max 15 points for posts within last week, declining over time
                if days_since_posted <= 1:
                    score += 15  # Posted within last day
                elif days_since_posted <= 3:
                    score += 12  # Posted within last 3 days
                elif days_since_posted <= 7:
                    score += 8   # Posted within last week
                elif days_since_posted <= 14:
                    score += 5   # Posted within last 2 weeks
                elif days_since_posted <= 30:
                    score += 2   # Posted within last month
            except:
                # Invalid date, skip recency bonus
                pass
        
        return round(score)
    
    def calculate_multi_keyword_score(self, job: Dict[str, Any], keywords: List[str], expected_salary: int = None) -> Dict[str, Any]:
        """Calculate relevance scores for multiple keywords and return the highest score."""
        if not keywords:
            logger.info("ðŸ” No keywords provided for scoring")
            return {'score': 0, 'best_keyword': '', 'all_scores': {}}
        
        all_scores = {}
        best_score = 0
        best_keyword = ''
        
        for keyword in keywords:
            if keyword.strip():
                score = self.calculate_relevance_score(job, keyword.strip(), expected_salary)
                all_scores[keyword] = score
                
                if score > best_score:
                    best_score = score
                    best_keyword = keyword
        
        return {
            'score': best_score,
            'best_keyword': best_keyword,
            'all_scores': all_scores
        }
    
    def _get_default_search_terms(self) -> List[str]:
        """Get default search terms from scraping defaults file."""
        try:
            defaults_file = "scraping_defaults.json"
            if os.path.exists(defaults_file):
                with open(defaults_file, 'r') as f:
                    data = json.load(f)
                    search_terms = data.get('search_terms', [])
                    if search_terms:
                        logger.info(f"ðŸ“ Loaded {len(search_terms)} default search terms: {', '.join(search_terms)}")
                        return search_terms
        except Exception as e:
            logger.error(f"Error loading default search terms: {e}")
        
        return []

    def evaluate_job_relevance_with_ai(self, job_title: str, job_description: str = None) -> str:
        """
        Use AI to evaluate job relevance and return one of four levels:
        'Highly Relevant', 'Somewhat Relevant', 'Somewhat Irrelevant', 'Irrelevant'

        If job_description is empty, use job_title only.
        """
        if not self.openai_client:
            return "AI Not Configured"

        try:
            # Ensure job_description is a string (handle case where it might be a list)
            if isinstance(job_description, list):
                job_description = ' '.join(str(item) for item in job_description if item)
            elif job_description is None:
                job_description = ""
            else:
                job_description = str(job_description)

            # Use description if available, otherwise fall back to title
            content_to_analyze = job_description.strip() if job_description and job_description.strip() else job_title.strip()

            if not content_to_analyze:
                return "No Content"

            # Create simplified prompt for GPT-5 Nano
            job_content = f"Title: {job_title}"
            if content_to_analyze != job_title:
                # Truncate description to keep token usage low
                description_short = content_to_analyze[:300]
                job_content += f"\nDescription: {description_short}"

            prompt = f"""Evaluate job relevance for product manager/engineer/software roles.

{job_content}

Rate as exactly one of: Highly Relevant, Somewhat Relevant, Somewhat Irrelevant, Irrelevant"""

            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "You are a job relevance evaluator. Respond with only the exact relevance level."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=15,
                temperature=0.1
            )

            relevance = response.choices[0].message.content.strip()

            # Validate response is one of the expected values
            valid_responses = ["Highly Relevant", "Somewhat Relevant", "Somewhat Irrelevant", "Irrelevant"]
            if relevance not in valid_responses:
                logger.warning(f"AI returned unexpected relevance: {relevance}")
                return "AI Error"

            return relevance

        except Exception as e:
            logger.error(f"Error in AI job relevance evaluation: {e}")
            return "AI Error"

    def create_filtered_jobs_csv(self, original_csv_path: str, user_id: str = None) -> str:
        """
        Create a filtered and scored CSV based on relevance criteria.

        Filtering rules:
        1. Remove jobs with Relevance_Score < 60
        2. Remove jobs where AI_Relevance = "Irrelevant"
        3. Exclude jobs with "president", "director", "VP" in title
        4. Boost scores based on AI_Relevance:
           - "Highly Relevant": +50
           - "Somewhat Relevant": +25
           - "Somewhat Irrelevant": +0
        5. Sort by final score (descending)
        """
        try:
            import pandas as pd

            # Read the original CSV
            df = pd.read_csv(original_csv_path)
            logger.info(f"ðŸ“Š Processing {len(df)} jobs for filtering...")

            # Initial count
            original_count = len(df)

            # Filter 1: Remove low relevance scores (< 60)
            df = df[df['Relevance_Score'] >= 60]
            after_score_filter = len(df)
            logger.info(f"ðŸ” After relevance score filter (â‰¥60): {after_score_filter} jobs remaining")

            # Filter 2: Remove "Irrelevant" AI evaluations
            df = df[df['AI_Relevance'] != 'Irrelevant']
            after_ai_filter = len(df)
            logger.info(f"ðŸ¤– After AI relevance filter (not Irrelevant): {after_ai_filter} jobs remaining")

            # Filter 3: Exclude executive positions
            exclusion_keywords = ['president', 'director', 'vp', 'vice president', 'chief', 'head of']
            title_mask = True
            for keyword in exclusion_keywords:
                title_mask = title_mask & (~df['Title'].str.contains(keyword, case=False, na=False))

            df = df[title_mask]
            after_title_filter = len(df)
            logger.info(f"ðŸš« After title exclusion filter: {after_title_filter} jobs remaining")

            if len(df) == 0:
                logger.warning("âš ï¸ No jobs remaining after filtering!")
                return None

            # Create enhanced score based on AI relevance
            def calculate_enhanced_score(row):
                base_score = row['Relevance_Score']
                ai_relevance = row['AI_Relevance']

                if ai_relevance == 'Highly Relevant':
                    return base_score + 50
                elif ai_relevance == 'Somewhat Relevant':
                    return base_score + 25
                elif ai_relevance == 'Somewhat Irrelevant':
                    return base_score  # No boost
                else:
                    return base_score  # For any unexpected values

            df['Enhanced_Score'] = df.apply(calculate_enhanced_score, axis=1)

            # Sort by enhanced score (descending)
            df = df.sort_values('Enhanced_Score', ascending=False)

            # Create filtered CSV filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filtered_csv_filename = f"jobspy_filtered_jobs_{timestamp}.csv"

            # Reorder columns to show Enhanced_Score prominently
            columns = ['Enhanced_Score', 'Company', 'Title', 'Location', 'Description',
                      'Salary_Min', 'Salary_Max', 'Salary_Interval', 'Currency',
                      'Date_Posted', 'Date_Scraped', 'Job_URL', 'Site', 'Job_Type',
                      'Is_Remote', 'Min_Experience_Years', 'Max_Experience_Years',
                      'Relevance_Score', 'Best_Matching_Keyword', 'AI_Relevance']

            # Ensure all columns exist (in case some are missing)
            available_columns = [col for col in columns if col in df.columns]
            df_filtered = df[available_columns]

            # Save filtered CSV
            df_filtered.to_csv(filtered_csv_filename, index=False)

            # Save filtered jobs to database
            self.save_filtered_jobs_to_database(df_filtered, user_id)

            # Log filtering summary
            logger.info(f"ðŸ“‹ FILTERING SUMMARY:")
            logger.info(f"   ðŸ“¥ Original jobs: {original_count}")
            logger.info(f"   ðŸŽ¯ After relevance score â‰¥60: {after_score_filter}")
            logger.info(f"   ðŸ¤– After AI relevance filter: {after_ai_filter}")
            logger.info(f"   ðŸš« After title exclusions: {after_title_filter}")
            logger.info(f"   ðŸ“¤ Final filtered list: {len(df_filtered)} jobs")
            logger.info(f"   ðŸ“ˆ Top enhanced score: {df_filtered['Enhanced_Score'].max():.0f}")
            logger.info(f"   ðŸ“‰ Lowest enhanced score: {df_filtered['Enhanced_Score'].min():.0f}")

            # Log top 3 jobs for preview
            logger.info(f"ðŸ† TOP 3 FILTERED JOBS:")
            for i, (_, job) in enumerate(df_filtered.head(3).iterrows()):
                logger.info(f"   {i+1}. {job['Title']} at {job['Company']} (Score: {job['Enhanced_Score']:.0f})")

            logger.info(f"ðŸ“„ Created filtered CSV: {filtered_csv_filename}")
            return filtered_csv_filename

        except Exception as e:
            logger.error(f"Error creating filtered CSV: {e}")
            return None

    def save_filtered_jobs_to_database(self, df_filtered, user_id: str = None):
        """Save filtered jobs to the FilteredJobView table for the UI."""
        try:
            from database import FilteredJobView, ScrapedJob
            from datetime import date
            import json

            if not user_id:
                logger.warning("No user_id provided for filtered jobs - skipping database save")
                return 0

            db = SessionLocal()
            try:
                today = date.today()
                saved_count = 0
                skipped_count = 0
                not_found_count = 0

                logger.info(f"ðŸ’¾ Saving {len(df_filtered)} filtered jobs to database...")

                for _, row in df_filtered.iterrows():
                    try:
                        # Find the corresponding ScrapedJob by matching URL and title
                        scraped_job = db.query(ScrapedJob).filter(
                            ScrapedJob.job_url == row.get('Job_URL'),
                            ScrapedJob.title == row.get('Title'),
                            ScrapedJob.company == row.get('Company')
                        ).first()

                        if not scraped_job:
                            # Try to find by title and company if URL doesn't match
                            scraped_job = db.query(ScrapedJob).filter(
                                ScrapedJob.title == row.get('Title'),
                                ScrapedJob.company == row.get('Company')
                            ).first()

                        if scraped_job:
                            # Check if this job is already in FilteredJobView for today for this user
                            existing_entry = db.query(FilteredJobView).filter(
                                FilteredJobView.user_id == user_id,
                                FilteredJobView.scraped_job_id == scraped_job.id,
                                FilteredJobView.filter_date == today
                            ).first()

                            if not existing_entry:
                                # Create filter criteria metadata
                                filter_criteria = {
                                    "min_relevance_score": 60,
                                    "ai_relevance_excluded": ["Irrelevant"],
                                    "excluded_titles": ["president", "director", "VP", "chief"],
                                    "enhanced_scoring": True
                                }

                                # Create FilteredJobView entry
                                filtered_view = FilteredJobView(
                                    user_id=user_id,
                                    scraped_job_id=scraped_job.id,
                                    scraping_run_id=scraped_job.scraping_run_id,
                                    filter_date=today,
                                    relevance_score=float(row.get('Relevance_Score', 0)),
                                    enhanced_score=float(row.get('Enhanced_Score', 0)),
                                    best_matching_keyword=row.get('Best_Matching_Keyword'),
                                    ai_relevance=row.get('AI_Relevance'),
                                    filter_criteria=filter_criteria
                                )

                                db.add(filtered_view)
                                saved_count += 1
                            else:
                                skipped_count += 1
                        else:
                            not_found_count += 1

                    except Exception as row_error:
                        logger.warning(f"âš ï¸  Error saving filtered job row: {row_error}")
                        continue

                db.commit()
                logger.info(f"âœ… Database save results: {saved_count} new, {skipped_count} already exist, {not_found_count} not found in scraped jobs")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"âŒ Error saving filtered jobs to database: {e}")

    def create_user_filtered_jobs(self, user_id: str, company_names: list, search_terms: list):
        """Create filtered jobs for a user from all available scraped jobs for the target companies."""
        try:
            from database import FilteredJobView, ScrapedJob
            from datetime import date, datetime, timezone, timedelta
            import pandas as pd

            logger.info(f"ðŸ”„ Creating filtered jobs for user {user_id}")

            db = SessionLocal()
            try:
                # Get jobs from the companies we're interested in (within last 30 days to include more data)
                recent_date = datetime.now(timezone.utc) - timedelta(days=30)

                # Build company filter
                company_filters = []
                for company_name in company_names:
                    company_filters.append(ScrapedJob.company.ilike(f"%{company_name}%"))

                # Get existing jobs for these companies
                existing_jobs = db.query(ScrapedJob).filter(
                    ScrapedJob.date_scraped >= recent_date,
                    *company_filters
                ).all()

                if not existing_jobs:
                    logger.info(f"ðŸ” No jobs found for companies: {', '.join(company_names)}")
                    return 0

                logger.info(f"ðŸ” Found {len(existing_jobs)} jobs to filter for user")

                # Convert jobs to DataFrame for filtering
                jobs_data = []
                for job in existing_jobs:
                    jobs_data.append({
                        'id': job.id,
                        'Company': job.company,
                        'Title': job.title,
                        'Location': job.location,
                        'Description': job.description or '',
                        'Salary_Min': job.min_amount,
                        'Salary_Max': job.max_amount,
                        'scraping_run_id': job.scraping_run_id
                    })

                if not jobs_data:
                    return 0

                df = pd.DataFrame(jobs_data)

                # Apply scoring and filtering
                scoring_keywords = [term for term in search_terms if term.lower() not in ['all']]

                # Apply relevance scoring - calculate best score across all search terms
                def calculate_best_score(row):
                    best_score = 0
                    best_keyword = ""
                    for search_term in scoring_keywords:
                        score = self.calculate_relevance_score(row.to_dict(), search_term, 0)
                        if score > best_score:
                            best_score = score
                            best_keyword = search_term
                    return best_score, best_keyword
                
                # Calculate scores and best keywords
                score_results = df.apply(calculate_best_score, axis=1)
                df['Relevance_Score'] = [result[0] for result in score_results]
                df['Best_Matching_Keyword'] = [result[1] for result in score_results]

                # Apply AI scoring if available
                if self.openai_client:
                    logger.info("ðŸ¤– Applying AI relevance evaluation...")
                    def safe_ai_evaluation(row):
                        try:
                            title = str(row['Title']) if row['Title'] is not None else ""
                            description = str(row['Description']) if row['Description'] is not None else ""
                            logger.debug(f"Evaluating: {title[:50]}...")
                            result = self.evaluate_job_relevance_with_ai(title, description)
                            logger.debug(f"AI result: {result}")
                            return result
                        except Exception as e:
                            logger.error(f"Error in AI evaluation for job '{row.get('Title', 'Unknown')}': {e}")
                            return "Error"

                    df['AI_Relevance'] = df.apply(safe_ai_evaluation, axis=1)
                    logger.info("ðŸ¤– AI evaluation completed")
                else:
                    df['AI_Relevance'] = 'Not Evaluated'

                # Filter jobs (score >= 60, not irrelevant)
                before_filter = len(df)
                df_filtered = df[
                    (df['Relevance_Score'] >= 60) &
                    (df['AI_Relevance'] != 'Irrelevant')
                ].copy()

                after_filter = len(df_filtered)
                logger.info(f"ðŸŽ¯ After filtering: {after_filter} jobs qualify (from {before_filter} total)")

                if len(df_filtered) == 0:
                    # Log why no jobs qualified
                    high_score_count = len(df[df['Relevance_Score'] >= 60])
                    ai_relevant_count = len(df[df['AI_Relevance'] != 'Irrelevant'])
                    logger.info(f"ðŸ“­ No jobs passed filtering criteria:")
                    logger.info(f"   - Jobs with score â‰¥60: {high_score_count}")
                    logger.info(f"   - Jobs not marked 'Irrelevant' by AI: {ai_relevant_count}")
                    logger.info(f"   - User search terms: {search_terms}")
                    return 0

                # Save to FilteredJobView
                today = date.today()
                saved_count = 0

                for _, row in df_filtered.iterrows():
                    # Check if this job is already in FilteredJobView for this user today
                    existing_entry = db.query(FilteredJobView).filter(
                        FilteredJobView.user_id == user_id,
                        FilteredJobView.scraped_job_id == row['id'],
                        FilteredJobView.filter_date == today
                    ).first()

                    if not existing_entry:
                        # Create new FilteredJobView entry
                        filtered_view = FilteredJobView(
                            user_id=user_id,
                            scraped_job_id=row['id'],
                            scraping_run_id=row['scraping_run_id'],
                            filter_date=today,
                            relevance_score=row['Relevance_Score'],
                            enhanced_score=row['Relevance_Score'],  # Can be enhanced later
                            best_matching_keyword=row['Best_Matching_Keyword'],
                            ai_relevance=row['AI_Relevance'],
                            filter_criteria={
                                'min_score': 60,
                                'search_terms': search_terms,
                                'companies': company_names
                            }
                        )

                        db.add(filtered_view)
                        saved_count += 1

                db.commit()
                logger.info(f"âœ… Created {saved_count} filtered job entries for user")
                return saved_count

            finally:
                db.close()

        except Exception as e:
            logger.error(f"âŒ Error creating filtered jobs for user: {e}")
            return 0

    def create_jobs_csv(self, scraping_run_id: int, user_id: str = None) -> str:
        """Create a CSV file with the jobs from the latest scraping run."""
        try:
            db = SessionLocal()
            try:
                from database import ScrapedJob
                
                # Get jobs from the specific scraping run
                jobs = db.query(ScrapedJob).filter(
                    ScrapedJob.scraping_run_id == scraping_run_id
                ).all()
                
                if not jobs:
                    logger.warning("No jobs found for CSV export")
                    return None
                
                # Get search terms from the scraping run parameters
                scoring_keywords = []
                expected_salary = 0

                # First try to get search terms from the scraping run
                scraping_run = db.query(ScrapingRun).filter(ScrapingRun.id == scraping_run_id).first()
                if scraping_run and scraping_run.search_parameters:
                    search_params = scraping_run.search_parameters
                    if isinstance(search_params, dict) and 'search_terms' in search_params:
                        scoring_keywords = search_params['search_terms']

                        # Filter out company names and "all" for scoring purposes
                        if isinstance(scoring_keywords, list):
                            company_names = search_params.get('company_names', [])
                            scoring_keywords = [
                                term for term in scoring_keywords
                                if term.lower() not in ['all'] and
                                term not in company_names and
                                term.lower() not in [name.lower() for name in company_names]
                            ]
                            logger.info(f"ðŸŽ¯ Filtered out company names and 'all' from scoring keywords")

                # Fall back to scoring config if no search terms found
                if not scoring_keywords:
                    scoring_config = self._get_scoring_config()
                    scoring_keywords = scoring_config.get('scoring_keywords', [])
                    expected_salary = scoring_config.get('expected_salary', 0)

                if scoring_keywords:
                    logger.info(f"ðŸ“Š Applying relevance scoring with keywords: {scoring_keywords}, expected salary: ${expected_salary:,}")
                else:
                    logger.info("ðŸ“Š No scoring keywords configured - all jobs will have score 0")

                # Count jobs by site and description availability for reporting
                site_counts = {}
                description_counts = {"with_description": 0, "without_description": 0}
                
                # Create CSV content
                output = io.StringIO()
                writer = csv.writer(output)
                
                # Write header with Relevance_Score, Best_Keyword, and AI_Relevance columns
                writer.writerow([
                    'Company', 'Title', 'Location', 'Description', 'Salary_Min', 'Salary_Max',
                    'Salary_Interval', 'Currency', 'Date_Posted', 'Date_Scraped', 'Job_URL',
                    'Site', 'Job_Type', 'Is_Remote', 'Min_Experience_Years', 'Max_Experience_Years',
                    'Relevance_Score', 'Best_Matching_Keyword', 'AI_Relevance'
                ])
                
                # Write job data with relevance scores
                jobs_with_scores = 0
                ai_evaluations_done = 0
                ai_evaluations_skipped = 0
                for job_index, job in enumerate(jobs):
                    # Convert job to dict for scoring function
                    job_dict = {
                        'title': job.title,
                        'description': job.description,
                        'company': job.company,
                        'job_type': job.job_type,
                        'min_amount': job.min_amount,
                        'max_amount': job.max_amount,
                        'date_posted': job.date_posted
                    }
                    
                    # Calculate relevance score for multiple keywords
                    relevance_score = 0
                    best_keyword = ''
                    if scoring_keywords:
                        scoring_result = self.calculate_multi_keyword_score(
                            job_dict, 
                            scoring_keywords, 
                            expected_salary if expected_salary > 0 else None
                        )
                        relevance_score = scoring_result['score']
                        best_keyword = scoring_result['best_keyword']
                        
                        # Log first few scores for debugging
                        if job_index < 3:
                            logger.info(f"ðŸ” Job {job_index+1}: '{job.title}' scored {relevance_score} (best: '{best_keyword}')")
                        if relevance_score > 0:
                            jobs_with_scores += 1

                    # Track statistics
                    site = job.site or 'unknown'
                    site_counts[site] = site_counts.get(site, 0) + 1
                    has_description = bool(job.description and job.description.strip())
                    if has_description:
                        description_counts["with_description"] += 1
                    else:
                        description_counts["without_description"] += 1

                    # Get AI relevance evaluation (only for jobs with good relevance scores)
                    ai_relevance = "Not Evaluated"

                    # Only do AI evaluation if job meets minimum relevance threshold
                    min_relevance_for_ai = 60  # Same threshold used for filtering

                    if relevance_score >= min_relevance_for_ai and self.openai_client:
                        try:
                            ai_relevance = self.evaluate_job_relevance_with_ai(
                                job.title or '',
                                job.description or ''
                            )
                            ai_evaluations_done += 1
                            # Log AI evaluation for first few jobs
                            if job_index < 3:
                                logger.info(f"ðŸ¤– AI evaluation for '{job.title}': {ai_relevance}")
                        except Exception as e:
                            logger.warning(f"AI evaluation failed for job {job_index+1}: {e}")
                            ai_relevance = "AI Error"
                    elif relevance_score < min_relevance_for_ai:
                        ai_relevance = "Low Relevance - Skipped"
                        ai_evaluations_skipped += 1
                    elif not self.openai_client:
                        if job_index == 0:  # Only log once
                            logger.warning("ðŸ¤– AI evaluation skipped - OpenAI client not configured")
                        ai_relevance = "AI Not Configured"

                    writer.writerow([
                        job.company or '',
                        job.title or '',
                        job.location or '',
                        (job.description or '').replace('\n', ' ').replace('\r', '') if job.description else '',
                        job.min_amount or '',
                        job.max_amount or '',
                        job.salary_interval or '',
                        job.currency or '',
                        job.date_posted.strftime('%Y-%m-%d') if job.date_posted else '',
                        job.date_scraped.strftime('%Y-%m-%d %H:%M:%S') if job.date_scraped else '',
                        job.job_url or '',
                        job.site or '',
                        job.job_type or '',
                        'Yes' if job.is_remote else 'No',
                        job.min_experience_years or '',
                        job.max_experience_years or '',
                        relevance_score,
                        best_keyword,
                        ai_relevance
                    ])
                
                # Save to temporary file
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                csv_filename = f"jobspy_daily_scraping_{timestamp}.csv"
                
                with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                    f.write(output.getvalue())
                
                # Log scoring summary if keywords were used
                ai_status = "with AI relevance evaluation" if self.openai_client else "without AI evaluation (OpenAI not configured)"
                logger.info(f"ðŸ¤– AI status check: openai_client = {self.openai_client is not None}, api_key_set = {bool(self.openai_api_key)}")
                # Log site and description statistics
                logger.info(f"ðŸŒ Jobs by site: {', '.join([f'{site}: {count}' for site, count in site_counts.items()])}")
                logger.info(f"ðŸ“ Description availability: {description_counts['with_description']} with descriptions, {description_counts['without_description']} without")

                if scoring_keywords:
                    logger.info(f"ðŸ“„ Created CSV export: {csv_filename} with {len(jobs)} jobs, multi-keyword relevance scores, {ai_status}")
                    logger.info(f"ðŸ“Š Scoring keywords used: {', '.join(scoring_keywords)}")
                    logger.info(f"ðŸ“Š Jobs with scores > 0: {jobs_with_scores} out of {len(jobs)}")
                    if self.openai_client:
                        logger.info(f"ðŸ¤– AI evaluations: {ai_evaluations_done} done, {ai_evaluations_skipped} skipped (optimization)")
                else:
                    logger.info(f"ðŸ“„ Created CSV export: {csv_filename} with {len(jobs)} jobs (no keyword scoring applied), {ai_status}")

                # Create filtered version if AI evaluation is available
                filtered_csv_filename = None
                if self.openai_client:
                    logger.info("ðŸŽ¯ Creating filtered and enhanced CSV...")
                    filtered_csv_filename = self.create_filtered_jobs_csv(csv_filename, user_id)

                # Return both filenames as a tuple
                return csv_filename, filtered_csv_filename
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"Error creating jobs CSV: {e}")
            return None
    
    def send_notification_email(self, summary: dict, csv_filename: str = None, filtered_csv_filename: str = None):
        """Send email notification with scraping results."""
        if not self.email_enabled or not self.notification_email:
            logger.info("ðŸ“§ Email notifications disabled or email not configured")
            return
        
        try:
            # Create message
            msg = MIMEMultipart()
            msg['From'] = self.email_user
            msg['To'] = self.notification_email
            msg['Subject'] = f"ðŸ¤– JobSpy Daily Scraping Report - {summary.get('date', 'Unknown')}"
            
            # Create email body
            body = f"""
JobSpy Daily Scraping Report
============================

ðŸ“Š SUMMARY:
â€¢ Total Companies Scraped: {summary.get('companies_scraped', 0)}
â€¢ Total Jobs Found: {summary.get('total_jobs', 0)}
â€¢ New Jobs Added: {summary.get('new_jobs', 0)}
â€¢ Duration: {summary.get('duration', 'Unknown')}
â€¢ Success Rate: {summary.get('success_rate', 'Unknown')}

ðŸ¢ COMPANIES:
{summary.get('company_details', 'No details available')}

ðŸ” SEARCH TERMS USED:
{', '.join(summary.get('search_terms', []))}

â° SCRAPING TIME:
Started: {summary.get('start_time', 'Unknown')}
Completed: {summary.get('end_time', 'Unknown')}

ðŸ“Ž ATTACHMENTS:
{"â€¢ Complete Job List: JobSpy_Daily_Jobs.csv - All scraped jobs with AI analysis" if csv_filename else ""}
{"â€¢ Filtered Job List: JobSpy_Filtered_Jobs.csv - High-quality jobs (score â‰¥60, no executives)" if filtered_csv_filename else ""}

---
Generated by JobSpy Automated Scraping System
            """.strip()
            
            msg.attach(MIMEText(body, 'plain'))
            
            # Attach original CSV file if provided
            if csv_filename and os.path.exists(csv_filename):
                with open(csv_filename, "rb") as attachment:
                    part = MIMEBase('application', 'octet-stream')
                    part.set_payload(attachment.read())

                encoders.encode_base64(part)
                # Create a more user-friendly filename for the attachment
                attachment_filename = f"JobSpy_Daily_Jobs_{summary.get('date', datetime.now().strftime('%Y-%m-%d'))}.csv"
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename="{attachment_filename}"'
                )
                msg.attach(part)
                logger.info(f"ðŸ“Ž Attached complete CSV: {attachment_filename} ({summary.get('total_jobs', 0)} jobs included)")

            # Attach filtered CSV file if provided
            if filtered_csv_filename and os.path.exists(filtered_csv_filename):
                with open(filtered_csv_filename, "rb") as attachment:
                    part = MIMEBase('application', 'octet-stream')
                    part.set_payload(attachment.read())

                encoders.encode_base64(part)
                # Create a more user-friendly filename for the filtered attachment
                filtered_attachment_filename = f"JobSpy_Filtered_Jobs_{summary.get('date', datetime.now().strftime('%Y-%m-%d'))}.csv"
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename="{filtered_attachment_filename}"'
                )
                msg.attach(part)
                logger.info(f"ðŸ“Ž Attached filtered CSV: {filtered_attachment_filename} (high-quality jobs only)")
            
            # Send email
            server = smtplib.SMTP(self.smtp_server, self.smtp_port)
            server.starttls()
            server.login(self.email_user, self.email_password)
            text = msg.as_string()
            server.sendmail(self.email_user, self.notification_email, text)
            server.quit()
            
            logger.info(f"ðŸ“§ Notification email sent to {self.notification_email}")
            
            # Clean up temporary CSV files
            if csv_filename and os.path.exists(csv_filename):
                os.remove(csv_filename)
                logger.info(f"ðŸ—‘ï¸ Cleaned up temporary file: {csv_filename}")

            if filtered_csv_filename and os.path.exists(filtered_csv_filename):
                os.remove(filtered_csv_filename)
                logger.info(f"ðŸ—‘ï¸ Cleaned up temporary filtered file: {filtered_csv_filename}")
            
        except Exception as e:
            logger.error(f"Failed to send notification email: {e}")
    
    async def _send_completion_notification(self, scraping_run, company_names, search_terms, start_time, end_time, duration, user_id: str = None):
        """Send notification email when scraping completes successfully."""
        try:
            # Get job statistics from the scraping run
            db = SessionLocal()
            try:
                from database import ScrapedJob
                
                total_jobs = db.query(ScrapedJob).filter(
                    ScrapedJob.scraping_run_id == scraping_run.id
                ).count()
                
                # Get job counts by company
                company_details = ""
                for company_name in company_names:
                    company_job_count = db.query(ScrapedJob).filter(
                        ScrapedJob.scraping_run_id == scraping_run.id,
                        ScrapedJob.company.ilike(f"%{company_name}%")
                    ).count()
                    company_details += f"â€¢ {company_name}: {company_job_count} jobs\n"
                
                # Create summary
                summary = {
                    'date': start_time.strftime('%Y-%m-%d'),
                    'companies_scraped': len(company_names),
                    'total_jobs': total_jobs,
                    'new_jobs': total_jobs,  # Assuming all jobs are new for now
                    'duration': f"{duration.total_seconds():.1f} seconds",
                    'success_rate': '100%',
                    'company_details': company_details.strip(),
                    'search_terms': search_terms,
                    'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S UTC'),
                    'end_time': end_time.strftime('%Y-%m-%d %H:%M:%S UTC')
                }
                
                # Create CSV export (both original and filtered versions)
                csv_result = self.create_jobs_csv(scraping_run.id, user_id)
                if isinstance(csv_result, tuple):
                    csv_filename, filtered_csv_filename = csv_result
                else:
                    # Backward compatibility if only one filename is returned
                    csv_filename = csv_result
                    filtered_csv_filename = None

                # Always create filtered jobs for the user from available data (existing + new)
                if user_id:
                    self.create_user_filtered_jobs(user_id, company_names, search_terms)

                # Create daily job review list
                await self._create_daily_review_list(start_time, total_jobs)

                # Send notification with both CSV files
                self.send_notification_email(summary, csv_filename, filtered_csv_filename)
                
                logger.info(f"ðŸ“§ Notification sent with {total_jobs} jobs from {len(company_names)} companies")
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"Failed to send completion notification: {e}")
    
    async def _create_daily_review_list(self, scraping_date, total_jobs_scraped):
        """Create a daily job review list after scraping completes."""
        try:
            # Only create review list if we scraped jobs
            if total_jobs_scraped > 0:
                target_date = scraping_date.strftime("%Y-%m-%d")
                
                db = SessionLocal()
                try:
                    review_list = daily_job_reviewer.create_daily_review_list(
                        target_date=target_date,
                        db=db,
                        force_recreate=True  # Recreate after new scraping
                    )
                    
                    if review_list:
                        logger.info(f"ðŸ“‹ Created daily review list for {target_date} with {review_list.jobs_selected_count} jobs")
                    else:
                        logger.warning(f"ðŸ“‹ No jobs qualified for daily review list on {target_date}")
                        
                finally:
                    db.close()
            else:
                logger.info("ðŸ“‹ Skipping daily review list creation - no jobs scraped")
                
        except Exception as e:
            logger.error(f"Failed to create daily review list: {e}")
    
    async def _send_failure_notification(self, error_message, start_time):
        """Send notification email when scraping fails."""
        try:
            summary = {
                'date': start_time.strftime('%Y-%m-%d'),
                'companies_scraped': 0,
                'total_jobs': 0,
                'new_jobs': 0,
                'duration': 'Failed',
                'success_rate': '0%',
                'company_details': f"âŒ Scraping failed with error: {error_message}",
                'search_terms': [],
                'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S UTC'),
                'end_time': 'N/A - Failed'
            }
            
            self.send_notification_email(summary)
            logger.info("ðŸ“§ Failure notification sent")
            
        except Exception as e:
            logger.error(f"Failed to send failure notification: {e}")
    
    def should_scrape_company(self, company: TargetCompany) -> bool:
        """Check if a company should be scraped based on last scrape time."""
        if not company.last_scraped:
            logger.info(f"Company '{company.name}' has never been scraped - will scrape")
            return True
        
        # Check if it's been more than 23 hours since last scrape (allow some buffer)
        # Handle timezone-naive datetime from database
        last_scraped = company.last_scraped
        if last_scraped.tzinfo is None:
            last_scraped = last_scraped.replace(tzinfo=timezone.utc)
        
        time_since_last_scrape = datetime.now(timezone.utc) - last_scraped
        should_scrape = time_since_last_scrape > timedelta(hours=23)
        
        if should_scrape:
            logger.info(f"Company '{company.name}' last scraped {time_since_last_scrape.total_seconds()/3600:.1f} hours ago - will scrape")
        else:
            logger.info(f"Company '{company.name}' was scraped recently - skipping")
        
        return should_scrape
    
    async def run_targeted_scraping(self, target_company_names, custom_search_terms=None, user_id: str = None, location=None, distance=None, max_results=None):
        """Execute scraping for specific companies."""
        if not self.enabled:
            logger.info("Auto-scraping is disabled")
            return
        
        start_time = datetime.now(timezone.utc)
        logger.info(f"ðŸŽ¯ Starting targeted scraping for {len(target_company_names)} companies at {start_time}")
        logger.info(f"ðŸ¢ Target companies: {', '.join(target_company_names)}")
        
        try:
            # Get all active companies from database
            db = SessionLocal()
            try:
                all_companies = db.query(TargetCompany).filter(
                    TargetCompany.is_active == True
                ).all()
                
                # Filter to only target companies (case-insensitive match)
                target_names_lower = [name.lower() for name in target_company_names]
                companies_to_scrape = []
                seen_names_lower = set()

                for company in all_companies:
                    if company.name.lower() in target_names_lower and company.name.lower() not in seen_names_lower:
                        companies_to_scrape.append(company)
                        seen_names_lower.add(company.name.lower())
                        logger.info(f"âœ… Found target company: {company.name}")
                
                # Check for companies not found in database and auto-create them
                found_names = [c.name.lower() for c in companies_to_scrape]
                missing_companies = [name for name in target_company_names 
                                   if name.lower() not in found_names]
                
                if missing_companies:
                    logger.info(f"ðŸ”§ {len(missing_companies)} companies not found in database, creating them automatically...")
                    for company_name in missing_companies:
                        new_company = self._create_target_company(db, company_name)
                        if new_company:
                            companies_to_scrape.append(new_company)
                            logger.info(f"âœ… Created and added target company: {new_company.name}")
                        else:
                            logger.warning(f"âŒ Failed to create company: {company_name}")
                
                if not companies_to_scrape:
                    logger.warning("âŒ No companies available for scraping (creation may have failed)")
                    return
                
                # Extract data while session is still active
                company_names = [company.name for company in companies_to_scrape]
                
                # Use custom search terms or company-specific terms
                search_terms = set()
                if custom_search_terms:
                    search_terms.update(custom_search_terms)
                    logger.info(f"ðŸ” Using custom search terms: {', '.join(custom_search_terms)}")
                else:
                    # Use company-specific search terms or defaults
                    for company in companies_to_scrape:
                        if company.search_terms:
                            search_terms.update(company.search_terms)
                        else:
                            search_terms.update(self.default_search_terms)
                    logger.info(f"ðŸ” Using default/company-specific search terms")

                
                search_terms = list(search_terms)
                    
            finally:
                db.close()
            
            logger.info(f"ðŸš€ Will scrape {len(company_names)} companies with {len(search_terms)} search terms")
            logger.info(f"ðŸ“ Search terms: {', '.join(search_terms[:5])}{'...' if len(search_terms) > 5 else ''}")
            
            # Get configuration values
            results_per_company = self._get_default_results_per_company()
            hours_old = self._get_default_hours_old()
            days_old = max(1, hours_old // 24)  # Convert hours to days

            # Create bulk scraping request
            scraping_request = BulkScrapingRequest(
                company_names=company_names,
                search_terms=search_terms,
                results_per_company=results_per_company,
                sites=["indeed", "linkedin"],
                locations=["USA"],
                job_types=[],
                days_old=days_old,
                is_remote=None,
                auto_scraping=True
            )
            
            # Execute scraping with progress logging
            db = SessionLocal()
            try:
                logger.info("â³ Starting job scraping process...")
                scraping_run = await self.job_scraper.bulk_scrape_companies(scraping_request, db)
                
                # Update last_scraped timestamp
                for company in companies_to_scrape:
                    company.last_scraped = datetime.now(timezone.utc)
                    db.merge(company)
                
                db.commit()
                
                end_time = datetime.now(timezone.utc)
                duration = end_time - start_time
                
                if scraping_run:
                    logger.info(f"âœ… Targeted scraping completed successfully!")
                    logger.info(f"â±ï¸  Duration: {duration.total_seconds():.1f} seconds")
                    logger.info(f"ðŸ“Š Run ID: {scraping_run.id}")
                    logger.info(f"ðŸŽ¯ Companies scraped: {len(company_names)}")
                    
                    # Send notification email with results
                    await self._send_completion_notification(scraping_run, company_names, search_terms, start_time, end_time, duration, user_id)
                else:
                    logger.error("âŒ Targeted scraping failed - no scraping run created")
                    
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"âŒ Targeted scraping failed: {str(e)}", exc_info=True)
            
            # Send failure notification
            await self._send_failure_notification(str(e), start_time if 'start_time' in locals() else datetime.now())
    
    async def run_daily_scraping(self):
        """Execute the daily scraping routine."""
        if not self.enabled:
            logger.info("Auto-scraping is disabled")
            return
        
        start_time = datetime.now(timezone.utc)
        logger.info(f"ðŸš€ Starting automated daily scraping at {start_time}")
        
        try:
            # Get active companies
            companies = self.get_active_companies()
            if not companies:
                logger.warning("No active target companies found - skipping scraping")
                return
            
            # Filter companies that need scraping
            companies_to_scrape = [c for c in companies if self.should_scrape_company(c)]
            
            if not companies_to_scrape:
                logger.info("No companies need scraping at this time")
                return
            
            # Prepare company names for bulk scraping
            company_names = [company.name for company in companies_to_scrape]
            
            # Determine search terms - use saved defaults, company-specific, or fallback defaults
            search_terms = set()
            
            # First try to get search terms from scraping defaults file
            default_search_terms = self._get_default_search_terms()
            if default_search_terms:
                search_terms.update(default_search_terms)
                logger.info(f"ðŸ” Using search terms from defaults: {', '.join(default_search_terms)}")
            else:
                # Fallback to company-specific or built-in defaults
                for company in companies_to_scrape:
                    if company.search_terms:
                        search_terms.update(company.search_terms)
                    else:
                        search_terms.update(self.default_search_terms)
                logger.info(f"ðŸ” Using fallback search terms")
            
            search_terms = list(search_terms)
            
            logger.info(f"Will scrape {len(company_names)} companies with {len(search_terms)} search terms")
            logger.info(f"Companies: {', '.join(company_names)}")
            logger.info(f"Search terms: {', '.join(search_terms)}")
            
            # Get configuration values
            results_per_company = self._get_default_results_per_company()
            hours_old = self._get_default_hours_old()
            days_old = max(1, hours_old // 24)  # Convert hours to days

            # Create bulk scraping request
            scraping_request = BulkScrapingRequest(
                company_names=company_names,
                search_terms=search_terms,
                results_per_company=results_per_company,
                sites=["indeed", "linkedin"],  # Default sites
                locations=["USA"],  # Default location
                job_types=[],  # All job types
                days_old=days_old,  # Jobs from configuration
                is_remote=None,  # Both remote and non-remote
                auto_scraping=True  # Flag to indicate this is auto-scraping
            )
            
            # Execute scraping
            db = SessionLocal()
            try:
                scraping_run = await self.job_scraper.bulk_scrape_companies(scraping_request, db)
                
                # Update last_scraped timestamp for scraped companies
                for company in companies_to_scrape:
                    company.last_scraped = datetime.now(timezone.utc)
                    db.merge(company)
                
                db.commit()
                
                end_time = datetime.now(timezone.utc)
                duration = end_time - start_time
                
                logger.info(f"âœ… Automated scraping completed successfully!")
                logger.info(f"   Duration: {duration.total_seconds():.1f} seconds")
                logger.info(f"   Scraping run ID: {scraping_run.id}")
                
                # Send notification email with results
                await self._send_completion_notification(scraping_run, company_names, search_terms, start_time, end_time, duration, None)
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"âŒ Automated scraping failed: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Send failure notification
            await self._send_failure_notification(str(e), start_time if 'start_time' in locals() else datetime.now())
    
    def schedule_daily_scraping(self):
        """Set up the daily scraping schedule for user-specific configurations only."""
        # Disable global scheduling completely - we only want user-specific scheduling
        logger.info("ðŸš« Global auto-scraping is DISABLED - using user-specific schedules only")

        # Schedule user-specific autoscraping
        self.schedule_user_autoscraping()

        # Also allow manual trigger via special time check
        schedule.every(1).minutes.do(self._check_manual_trigger)

    def schedule_user_autoscraping(self):
        """Set up scheduling for all enabled user-specific autoscraping configurations."""
        db = SessionLocal()
        try:
            # Get all enabled user autoscraping configurations
            user_configs = db.query(UserAutoscrapingConfig).filter(
                UserAutoscrapingConfig.enabled == True
            ).all()

            logger.info(f"ðŸ¤– Found {len(user_configs)} enabled user autoscraping configurations")

            for config in user_configs:
                # Schedule each user's autoscraping at their specified time
                schedule_time = config.schedule_time or "02:00"

                # Create a closure to capture the user_id for this specific schedule
                def create_user_scraping_job(user_id: str, config_id: int):
                    return lambda: asyncio.run(self.run_user_autoscraping(user_id, config_id))

                schedule.every().day.at(schedule_time).do(
                    create_user_scraping_job(config.user_id, config.id)
                )

                user = db.query(User).filter(User.id == config.user_id).first()
                username = user.username if user else f"user_{config.user_id}"

                logger.info(f"ðŸ“… USER-SPECIFIC: {username} scheduled at {schedule_time}")

                # Log user's configuration
                companies = config.companies or []
                company_names = [c.get("name") for c in companies if c.get("active", True)]
                search_terms = config.search_terms or ["software engineer"]

                logger.info(f"   ðŸ“‹ USER COMPANIES: {company_names}")
                logger.info(f"   ðŸ” USER SEARCH TERMS: {search_terms}")

        except Exception as e:
            logger.error(f"Error setting up user autoscraping schedules: {e}")
        finally:
            db.close()

    async def run_user_autoscraping(self, user_id: str, config_id: int):
        """Run autoscraping for a specific user configuration - COMPLETELY USER-SPECIFIC."""
        db = SessionLocal()
        try:
            # Get the user's autoscraping configuration
            config = db.query(UserAutoscrapingConfig).filter(
                UserAutoscrapingConfig.id == config_id,
                UserAutoscrapingConfig.user_id == user_id,
                UserAutoscrapingConfig.enabled == True
            ).first()

            if not config:
                logger.warning(f"User autoscraping config {config_id} for user {user_id} not found or disabled")
                return

            user = db.query(User).filter(User.id == user_id).first()
            username = user.username if user else f"user_{user_id}"

            logger.info(f"ðŸš€ STARTING USER-SPECIFIC AUTOSCRAPING for {username}")

            # Extract user-specific configuration (NO GLOBAL DEFAULTS)
            companies = config.companies or []
            company_names = [c.get("name") for c in companies if c.get("active", True)]
            search_terms = config.search_terms or []

            if not company_names:
                logger.warning(f"âŒ No companies configured for user {username} - skipping")
                return

            if not search_terms:
                logger.warning(f"âŒ No search terms configured for user {username} - skipping")
                return

            logger.info(f"ðŸŽ¯ USER {username}: COMPANIES = {company_names}")
            logger.info(f"ðŸ” USER {username}: SEARCH TERMS = {search_terms}")
            logger.info(f"ðŸ“ USER {username}: LOCATION = {config.location or 'Not specified'}")

            # Use user-specific scraping parameters
            location = config.location or "USA"
            distance = config.distance or 25
            max_results = config.max_results or 100

            logger.info(f"âš™ï¸ USER {username}: LOCATION={location}, DISTANCE={distance}, MAX_RESULTS={max_results}")

            # Run targeted scraping with user's specific settings
            await self.run_targeted_scraping(
                target_company_names=company_names,
                custom_search_terms=search_terms,
                user_id=user_id,
                location=location,
                distance=distance,
                max_results=max_results
            )

            logger.info(f"âœ… COMPLETED USER-SPECIFIC AUTOSCRAPING for {username}")

        except Exception as e:
            logger.error(f"âŒ ERROR in user autoscraping for {username}: {e}")
        finally:
            db.close()
    
    def _check_manual_trigger(self):
        """Check for manual trigger file to run scraping immediately."""
        import tempfile
        trigger_file = os.path.join(tempfile.gettempdir(), "trigger_scraping")
        if os.path.exists(trigger_file):
            logger.info("ðŸ”„ Manual trigger detected - running scraping now")
            try:
                # Read trigger data to get specific companies/search terms
                trigger_data = {}
                try:
                    with open(trigger_file, 'r') as f:
                        import json
                        trigger_data = json.load(f)
                except:
                    # Fallback for old format
                    trigger_data = {}
                
                os.remove(trigger_file)
                
                # Run scraping with specific parameters if provided
                company_names = trigger_data.get("company_names", [])
                search_terms = trigger_data.get("search_terms", [])
                
                if company_names:
                    asyncio.run(self.run_targeted_scraping(company_names, search_terms))
                else:
                    asyncio.run(self.run_daily_scraping())
                    
            except Exception as e:
                logger.error(f"Manual scraping failed: {e}")
    
    def start_scheduler(self):
        """Start the background scheduler."""
        if self.is_running:
            logger.warning("Scheduler is already running")
            return
        
        self.is_running = True
        self.schedule_daily_scraping()
        
        def run_scheduler():
            logger.info("ðŸŽ¯ Scheduler thread started")
            while self.is_running:
                schedule.run_pending()
                time.sleep(60)  # Check every minute
            logger.info("ðŸ›‘ Scheduler thread stopped")
        
        self.scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("âœ… Auto-scraping scheduler started successfully")
    
    def stop_scheduler(self):
        """Stop the background scheduler."""
        if not self.is_running:
            return
        
        self.is_running = False
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)
        
        schedule.clear()
        logger.info("ðŸ›‘ Auto-scraping scheduler stopped")
    
    def get_next_run_time(self) -> Optional[datetime]:
        """Get the next scheduled scraping time."""
        jobs = schedule.get_jobs()
        if jobs:
            next_run = min(job.next_run for job in jobs if job.next_run)
            return next_run
        return None
    
    def get_status(self) -> dict:
        """Get scheduler status information."""
        next_run = self.get_next_run_time()
        active_companies = len(self.get_active_companies())

        # Get user autoscraping configurations
        user_configs = []
        db = SessionLocal()
        try:
            user_autoscraping_configs = db.query(UserAutoscrapingConfig).filter(
                UserAutoscrapingConfig.enabled == True
            ).all()

            for config in user_autoscraping_configs:
                user = db.query(User).filter(User.id == config.user_id).first()
                companies = config.companies or []
                company_names = [c.get("name") for c in companies if c.get("active", True)]

                user_configs.append({
                    "user_id": config.user_id,
                    "username": user.username if user else f"user_{config.user_id}",
                    "schedule_time": config.schedule_time,
                    "companies": company_names,
                    "search_terms": config.search_terms or [],
                    "enabled": config.enabled
                })
        except Exception as e:
            logger.error(f"Error getting user autoscraping configs: {e}")
        finally:
            db.close()

        return {
            "enabled": self.enabled,
            "running": self.is_running,
            "schedule_time": self.schedule_time,
            "next_run": next_run.isoformat() if next_run else None,
            "active_companies_count": active_companies,
            "max_results_per_company": self._get_default_results_per_company(),
            "hours_old": self._get_default_hours_old(),
            "default_search_terms": self.default_search_terms,
            "user_configurations": user_configs
        }

# Global scheduler instance
auto_scraper = AutoScrapingScheduler()

def start_auto_scraping():
    """Start the automatic scraping service."""
    auto_scraper.start_scheduler()

def stop_auto_scraping():
    """Stop the automatic scraping service."""
    auto_scraper.stop_scheduler()

def trigger_manual_scraping(company_names=None, search_terms=None):
    """Trigger manual scraping by creating trigger file with optional parameters."""
    try:
        import tempfile
        trigger_data = {
            "timestamp": str(datetime.now()),
            "company_names": company_names or [],
            "search_terms": search_terms or []
        }
        
        trigger_file = os.path.join(tempfile.gettempdir(), "trigger_scraping")
        with open(trigger_file, "w") as f:
            import json
            json.dump(trigger_data, f)
        
        if company_names:
            logger.info(f"âœ… Manual scraping trigger created for companies: {', '.join(company_names)}")
        else:
            logger.info("âœ… Manual scraping trigger created for all companies")
        logger.info(f"ðŸ“ Trigger file location: {trigger_file}")
        return True
    except Exception as e:
        logger.error(f"Failed to create manual trigger: {e}")
        return False

def get_scheduler_status():
    """Get current scheduler status."""
    return auto_scraper.get_status()