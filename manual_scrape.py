#!/usr/bin/env python3
"""
Manual Job Scraping Script

Use this script to quickly scrape jobs for specific companies
without using the web interface or API calls.
"""

import asyncio
import sys
import os
from datetime import datetime

# Add backend directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from database import get_db
from job_scraper import job_scraper
from models import BulkScrapingRequest


async def scrape_companies(company_names, search_terms=None, locations=None, results_per_company=100):
    """Scrape jobs for specified companies."""
    
    if not search_terms:
        search_terms = []  # Empty list means use comprehensive default terms
    
    if not locations:
        locations = ["USA", "Remote"]
    
    print(f"ğŸš€ Starting scraping for {len(company_names)} companies...")
    print(f"ğŸ“ Companies: {', '.join(company_names)}")
    search_terms_display = ', '.join(search_terms) if search_terms else "COMPREHENSIVE (6 common job types)"
    print(f"ğŸ” Search terms: {search_terms_display}")
    print(f"ğŸ“ Locations: {', '.join(locations)}")
    print(f"ğŸ“Š Results per company: {results_per_company}")
    print()
    
    # Create scraping request
    request = BulkScrapingRequest(
        company_names=company_names,
        search_terms=search_terms,
        sites=["indeed"],  # Using Indeed for reliability
        locations=locations,
        results_per_company=results_per_company,
        hours_old=720  # Last 30 days
    )
    
    # Get database session
    db = next(get_db())
    
    try:
        # Run the scraping
        scraping_run = await job_scraper.bulk_scrape_companies(request, db)
        
        # Show results
        print("\n" + "="*50)
        print("ğŸ‰ SCRAPING COMPLETED!")
        print("="*50)
        print(f"âœ… Status: {scraping_run.status}")
        print(f"ğŸ“Š Total jobs found: {scraping_run.total_jobs_found}")
        print(f"â• New jobs added: {scraping_run.new_jobs_added}")
        print(f"ğŸ”„ Duplicates skipped: {scraping_run.duplicate_jobs_skipped}")
        print(f"â±ï¸  Duration: {scraping_run.duration_seconds} seconds")
        print(f"ğŸ†” Run ID: {scraping_run.id}")
        
        if scraping_run.error_message:
            print(f"âš ï¸  Errors: {scraping_run.error_message}")
        
        print("\nğŸ” You can now search these jobs using:")
        print("   â€¢ Web interface: http://localhost:8000")
        print("   â€¢ API: POST /search-jobs-local-public")
        print("   â€¢ Test search script: python test_search.py")
        
        return True
        
    except Exception as e:
        print(f"âŒ Scraping failed: {e}")
        return False
    finally:
        db.close()


def main():
    """Main function with predefined company lists."""
    print("ğŸ¤– Manual Job Scraping Script")
    print("="*40)
    
    # Predefined company sets
    company_sets = {
        "1": {
            "name": "Big Tech (FAANG+)",
            "companies": ["Google", "Apple", "Microsoft", "Amazon", "Meta", "Netflix", "Tesla"]
        },
        "2": {
            "name": "Hot Startups",
            "companies": ["OpenAI", "Stripe", "Coinbase", "Databricks", "Snowflake", "Palantir", "Uber"]
        },
        "3": {
            "name": "Enterprise Software",
            "companies": ["Salesforce", "Adobe", "Oracle", "SAP", "ServiceNow", "Workday", "Atlassian"]
        },
        "4": {
            "name": "Financial Tech",
            "companies": ["JPMorgan", "Goldman Sachs", "Robinhood", "Plaid", "Square", "PayPal", "Visa"]
        },
        "5": {
            "name": "Custom (Enter your own)"
        }
    }
    
    print("\nChoose a company set to scrape:")
    for key, value in company_sets.items():
        print(f"  {key}. {value['name']}")
        if 'companies' in value:
            print(f"     {', '.join(value['companies'][:3])}{'...' if len(value['companies']) > 3 else ''}")
    
    choice = input("\nEnter your choice (1-5): ").strip()
    
    if choice in company_sets and 'companies' in company_sets[choice]:
        companies = company_sets[choice]['companies']
        print(f"\nâœ… Selected: {company_sets[choice]['name']}")
    elif choice == "5":
        companies_input = input("\nEnter company names (comma-separated): ").strip()
        companies = [c.strip() for c in companies_input.split(',') if c.strip()]
        if not companies:
            print("âŒ No companies entered. Exiting.")
            return
    else:
        print("âŒ Invalid choice. Exiting.")
        return
    
    # Ask for search terms
    use_default_terms = input(f"\nUse default search terms (software engineer, data scientist, product manager)? (y/n): ").lower().strip()
    
    if use_default_terms == 'y':
        search_terms = None  # Will use defaults
    else:
        terms_input = input("Enter search terms (comma-separated): ").strip()
        search_terms = [t.strip() for t in terms_input.split(',') if t.strip()]
        if not search_terms:
            search_terms = None
    
    # Ask for results per company
    try:
        results_input = input(f"\nResults per company (default 100): ").strip()
        results_per_company = int(results_input) if results_input else 100
    except ValueError:
        results_per_company = 100
    
    print(f"\nğŸš€ Starting scraping...")
    
    # Run the scraping
    success = asyncio.run(scrape_companies(
        company_names=companies,
        search_terms=search_terms,
        results_per_company=results_per_company
    ))
    
    if success:
        print("\nâœ… Scraping completed successfully!")
    else:
        print("\nâŒ Scraping failed. Check the error messages above.")


if __name__ == "__main__":
    main()